{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Time Warping for Speech Recognition\n",
    "Chloé Clavel / Matthieu Labeau (original lab by Olivier Cappé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The directory ```DATA``` contains three directories :\n",
    "- ```SIG``` contains 12 repetitions of numbers from 0 to 9 by speaker1 (F e = 8000kHz). SIG_Rep_5_Number_3.wav is the recording corresponding to the fifth repetition of the number “3” . The first four repetitions of each number are represented on Figure 1.\n",
    "- ```SIG2```, contains 6 repetitions of numbers from 0 to 9 by speaker2\n",
    "- ```SIG_MIXED``` contains 6 repetitions of the number by speaker1 (SIG_MIXED_Rep_1:6) and then by speaker2 ( SIG_MIXED_Rep_7:12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "file_path = \"./SIG/SIG_Rep_1_Number_0.wav\"\n",
    "sample_rate, wav_data = wavfile.read(file_path)\n",
    "plt.plot(wav_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.fft.ifft(np.log(np.abs(np.fft.fft(wav_data)))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.specgram(wav_data, Fs=sample_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.hanning(256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Feature Extraction\n",
    "\n",
    "Write a python function ```feature_extraction``` to calculate the cepstral parameterization associated with each pronunciation. With regard to the parameters of the analysis, we will make the following choices: \n",
    "- **Parameterization type**: Cepstrum (real number) using linear frequency scale (as opposed to Mel-frequency cepstrum) and computed using FFT (Fast Fourier Transform)\n",
    "- **Weigthing window**: Hanning\n",
    "\n",
    "The inputs of the function will be the following:\n",
    "- **Wav data**: The data extracted from a .wav file\n",
    "- **Window size**: 256 samples (32 ms)\n",
    "- **Window shift**: 128 samples\n",
    "- **Cepstral order**: p = 10\n",
    "\n",
    "\n",
    "For each signal, the result of the analysis is a sequence of parameters that will be stored in a matrix\n",
    "according to the following convention:\n",
    "\n",
    "$$ \\quad X = \\underbrace{\\left[\\begin{array}{c} \n",
    " X_1 \\\\\n",
    " X_2 \\\\\n",
    " \\vdots \\\\\n",
    " X_{l_X}\n",
    "\\end{array} \\right]}_p  $$\n",
    "\n",
    "Each $X_i$ is a line vector of size $p$. Note that the number of ceptral vectors obtained ($l_X$) depends on the length of the signal, so it is variable. We obtain one matrix of this type per signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(wav_data, window_size, window_shift, cepstral_order):\n",
    "    #\n",
    "    # Indication: use the numpy functions np.hanning, np.fft.fft, and np.fft.ifft.\n",
    "    # Use the attribute .real to take the real part of complex numbers that are output by np.fft.ifft\n",
    "    # Don't hesitate to look at the numpy documentation to get a better understanding of what the functions do !\n",
    "    (np.fft.ifft(variable)).re\n",
    "    \n",
    "    return cepstral_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example !\n",
    "window_size = 256\n",
    "window_shift = 128\n",
    "cepstral_order = 10\n",
    "cepstral_vectors = feature_extraction(wav_data, window_size, window_shift, cepstral_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5098 // 128 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of cepstral matrix:\", [len(cepstral_vectors), len(cepstral_vectors[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Alignment by Dynamic Programming\n",
    "\n",
    "Write a python function ```time_alignment(x, y, gamma)``` performing time alignment by dynamic programming between two sequences of vectors stored with the convention above. The Euclidean norm between the following vectors and time\n",
    "constraints will be used:\n",
    "\n",
    "\\begin{matrix}\n",
    "  \\bullet & \\rightarrow & \\bullet\\\\\n",
    "   & \\nearrow_{\\gamma} & \\uparrow\\\\\n",
    "  \\bullet & & \\bullet\n",
    "\\end{matrix}\n",
    "\n",
    "with a diagonal weighting $\\gamma$ that may be chosen different from 1. It is essential to carry out this function in a parameterized way in order to be able to validate it on the mono-dimensional example being studied.\n",
    "The function will take as **inputs** the two vector sequences $(X_1, \\dots, X_{l_X})$ et $(Y_1, \\dots, Y_{l_Y})$ stored in the two matrices, as well as the parameter $\\gamma$. It will then execute the following operations:\n",
    "- **Calculation of the similarity matrix**: Given the two vector sequences, calculate the matrix $D (l_X \\times l_Y)$ such that $D(i,j) = \\|X_i-Y_j\\|$.\n",
    "- **Initialization of the cumulative distance matrix**: Calculate the content of the first row and column of the cumulative distance matrix $C (l_X \\times l_Y)$ such that $C(i,j)$ contains the cumulative distance along the minimum cumulative distance path joining the node $(i,j)$ at the initial node $(1,1)$.\n",
    "- **Calculation of the matrix of cumulative distances**: For $i$ ranging from 2 to $\\min(l_X,l_Y)$, calculation of $C(i,i)$ then $C(i+1,i), \\dots C(l_X,i)$ and $C(i,i+1), C(i,i+2), \\dots C(i,l_Y)$ (you will recall what justifies this way to proceed). To store the predecessor along the optimal path leading to $(i,j)$, you can use a three-dimensional array $B (l_X \\times l_Y \\times 2)$ since the predecessor is a node of the network defined by its two coordinates (we can alternatively use a \"code\" representing the three possible predecessors).\n",
    "- ***Bactracking***: From the node $(l_X,l_Y)$, reconstruction of the optimal path (starting from the end).\n",
    "\n",
    "Finally, the **output** will be the cumulative distance along the path of least cost  (```distance```) and the alignment path (```M```), in the form of the matrix  $M$  with two columns such that $X_{M(i,1)}$  is the vector mapped to $Y_{M(i,2)}$ (note that the number of lines in this matrix corresponding to the length of the optimal path is variable). The function can also return the matrix of cumulative distances (```C```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def time_alignment(x, y, gamma):\n",
    "    #\n",
    "    # \n",
    "    #\n",
    "    return C, M, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of time alignment\n",
    "file_path_1 = \"./SIG/SIG_Rep_1_Number_0.wav\"\n",
    "sample_rate_1, wav_data_1 = wavfile.read(file_path_1)\n",
    "file_path_2 = \"./SIG/SIG_Rep_2_Number_0.wav\"\n",
    "sample_rate_2, wav_data_2 = wavfile.read(file_path_2)\n",
    "cepstral_vectors_1 = feature_extraction(\n",
    "wav_data_1, window_size, window_shift, cepstral_order)\n",
    "cepstral_vectors_2 = feature_extraction(\n",
    "wav_data_2, window_size, window_shift, cepstral_order)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "C, M, distance = time_alignment(cepstral_vectors_1, cepstral_vectors_2, gamma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of visualization of the optimal path\n",
    "M = np.array(M)\n",
    "plt.imshow(D, cmap='jet')\n",
    "plt.title(\"Optimal path\")\n",
    "plt.plot(M[:,1], M[:,0])\n",
    "plt.show()\n",
    "print(\"Cost of the optimal path:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Speech Data\n",
    "\n",
    "First, working on the data from ```SIG```, you will align a reference sequence (for example, the one corresponding to the first repetition of the first word) with all the others. If we try to find, among all numbers of a given repetition (from 1 to 12), which is the same one as the reference, by looking at the closest in terms of distance (of the path of least cost):\n",
    "1. Are the performances satisfactory ?\n",
    "2. What is the influence of the γ choice (weighting of the diagonal path) on the alignment paths (in order to answer, examine characteristic cases and try to judge the relevance of the alignment paths by visualizing them) ? Does this parameter have a significant influence on the obtained distance?\n",
    "3. What implicit hypothesis is made when using the Euclidean distance to measure the proximity of ceptral vectors? Does this hypothesis seem acceptable to you in view of the differences between aligned sequences corresponding to different repetitions of the same word? Propose (it is not necessary to implement it!) a method to estimate from the aligned data an appropriate weighting for the comparison of ceptral vectors. Do you think this weighting could improve discrimination performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Speech Recognition\n",
    "\n",
    "The goal of this part is to implement different cross-validation strategies in order to evaluate how well our method can be used for speech recognition, especially when we generalize to more than one speaker. \n",
    "We will separate the available signals into a *reference* and *unknown* set, and label all the unknow signals by finding the signal in the reference set they are the closest to (in terms of cost of the optimal path given by ```time_alignement```. We will then rotate the reference/unknown sets to perform cross-validation. \n",
    "To do so, write a function ```evaluation_recognition```. For example, you can use the following formalism : \n",
    "\n",
    "```[confusion, accuracy, D] = evaluation_recognition(vectors, gamma, protocol)```\n",
    "\n",
    "- ```vectors``` contains all the cepstral vectors for the signals in the database. It can be under the form of a list of matrices, being the outputs of ```feature_extraction``` for all signals. If $N$ if the number of repetetitions, ```vectors``` contains $10N$ matrices.\n",
    "- ```gamma``` is the parameter $\\gamma$ of the alignment.\n",
    "- ```protocol``` is an integer value between 1 and 3 indicating the protocol to be used: each protocol corresponds to a particular way of dividing signals into training and testing sets. \n",
    "\n",
    "\n",
    "- ```confusion``` is the confusion matrix of size $10 \\times 10$.\n",
    "- ```accuracy``` is the recognition rate.\n",
    "- ```D``` is the distance matrix between each of the elements of the database of size $10N \\times 10N$, where $N$ is the number of repetitions. Note that only parts of ```D``` will be used, depending on the protocol choosed. \n",
    "\n",
    "The three proposed evaluation protocols are:\n",
    "- **Protocol 1**: The database is randomly divided in three subsets. Two of these subsets (2/3 of repetitions) are used for the reference set and the remaining subset (1/3 of repetitions) is used for the unknown set. The procedure is repeated by rotating the unknown and reference sets.\n",
    "- **Protocol 2**: A database containing only one pronunciation of each number is used as a reference database. The remaining repetitions are used for the unkown set. The procedure is repeated by rotating the unkown and reference sets. This protocol aims at evaluating the generalization capabilities of the recognition system. \n",
    "- **Protocol 3**: The first half of the database of ```SIG_MIXED``` is used as a reference database and the second half as the unkown database. The procedure is repeated by inversing the role of the two sets. This protocol aims at evaluating the generalization capabilities on a database containing data from the two speakers. Careful: since the goal here is to understand if you can use examples from one person to recognize numbers pronounced by a different person, you have to not shuffle the examples !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "- Evaluate the speech recognition performance with protocol 1 on the data ```SIG```, and then on the data ```SIG2```. What are the confusions ? What value of $\\gamma$ parameter offers the best results ?\n",
    "- Evaluate the generalization capabilities using protocol 2 on the data ```SIG```, and then on the data ```SIG2```. What do you notice?\n",
    "- Evaluate the generalization capabilities to other speakers using protocol 3 on ```SIG_MIXED```. Explain the results.\n",
    "- What are the limitations of dynamic time warping approaches for speech recognition that you have identified during this lab ? Propose possible solutions to these problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
